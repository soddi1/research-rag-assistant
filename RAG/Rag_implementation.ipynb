{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain \n",
    "import langchain_community\n",
    "import langchain_huggingface\n",
    "import langchain_pinecone \n",
    "import pinecone\n",
    "import dotenv\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables are saved to .env file.\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACE_API_KEY = \"enter_your_api_key\"\n",
    "PINECONE_API_KEY = \"enter_your_api_key\"\n",
    "\n",
    "env_content = f\"\"\"\n",
    "HUGGINGFACE_API_KEY= \n",
    "PINECONE_API_KEY= \n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as file:\n",
    "    file.write(env_content)\n",
    "\n",
    "print(\"Environment variables are saved to .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Environment File\n",
    "\n",
    "Run the following snippet of code to load the environment file each time you use this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from uuid import uuid4\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(pc):\n",
    "    index_name = \"chatbotresearch\"\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    return index_name\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def print_question(question):\n",
    "    print(question)\n",
    "    return question\n",
    "\n",
    "def check_string(input_string):\n",
    "    match = re.search(r'`(.*?)`', input_string)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchChatbot:\n",
    "    def __init__(self):\n",
    "        self.pdfs = []\n",
    "        self.documents = []\n",
    "        self.pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "        self.index_name = create_index(self.pc)\n",
    "        self.retriver = None\n",
    "        self.wiki_retriever_instance = WikipediaRetriever()\n",
    "        self.local_cache = {}\n",
    "        def wiki_retriever(query):\n",
    "            return self.wiki_retriever_instance.invoke(query)\n",
    "\n",
    "        repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        GOOGLE_API_KEY=\"AIzaSyAVOt9dfM1lwJg-FGiXivRBsdVlNYn5mos\"\n",
    "        self.llm_setup = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY, temperature=0.2)\n",
    "\n",
    "        self.llm = HuggingFaceEndpoint(\n",
    "            repo_id=repo_id,\n",
    "            temperature= 0.8,\n",
    "            top_k= 50,\n",
    "            huggingfacehub_api_token=os.getenv('HUGGINGFACE_API_KEY')\n",
    "        )\n",
    "\n",
    "    def load_papers(self, filepath):\n",
    "        for f in os.listdir(filepath):\n",
    "            if f.endswith('.pdf'):\n",
    "                self.pdfs.append(f)\n",
    "\n",
    "        for pdf in self.pdfs:\n",
    "            loader = PyMuPDFLoader(os.path.join(filepath, pdf))\n",
    "            self.documents.extend(loader.load())\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        self.docs = text_splitter.split_documents(self.documents)\n",
    "        return\n",
    "\n",
    "    def store_papers(self):\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "        index = self.pc.Index(self.index_name)\n",
    "\n",
    "        vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "        uuids = [str(uuid4()) for _ in range(len(self.documents))]\n",
    "        vector_store.add_documents(documents=self.documents, ids=uuids)\n",
    "\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"k\": 2, \"score_threshold\": 0.5},\n",
    "        )\n",
    "        \n",
    "    def generate(self, query):\n",
    "\n",
    "        # def llm(query):\n",
    "        #     return self.llm_setup.invoke(str(query)).content\n",
    "        \n",
    "        if query in self.local_cache:\n",
    "                self.local_cache[query] = self.local_cache.pop(query)\n",
    "                return self.local_cache[query]\n",
    "        if not self.local_cache:\n",
    "            template2 = \"\"\"You are given a question and its context. If the context and question do not match in terms of the topic, then ignore the context and answer to the best of your knowledge. Otherwise, answer the question using the context provided.\n",
    "                \n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Answer:\"\"\"\n",
    "            prompt2 = PromptTemplate(\n",
    "                template=template2,\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "            gen_chain = prompt2 | self.llm | StrOutputParser()\n",
    "\n",
    "            template3 = \"\"\"Using the following context answer the question. Do not make stuff from your on.\n",
    "                \n",
    "            Question: {question}\n",
    "            Context: {wiki_context}\n",
    "            Answer:\"\"\"\n",
    "            prompt3 = PromptTemplate(\n",
    "                template=template3,\n",
    "                input_variables=[\"wiki_context\", \"question\"]\n",
    "            )\n",
    "            wiki_chain = prompt3 | self.llm | StrOutputParser()\n",
    "            \n",
    "            condition = check_string(query)\n",
    "            if condition == \"No\":\n",
    "                # print(\"General/Context\")\n",
    "                article = self.retriever.invoke(query)\n",
    "                inputs = {\n",
    "                    \"context\": article,\n",
    "                    \"question\": query\n",
    "                }\n",
    "                self.local_cache[query] = gen_chain.invoke(inputs)\n",
    "                return gen_chain.invoke(inputs)\n",
    "            else:\n",
    "                # print(\"Wikipedia\")\n",
    "                article = self.wiki_retriever_instance.invoke(condition)\n",
    "                inputs = {\n",
    "                    \"wiki_context\": article,\n",
    "                    \"question\": query\n",
    "                }\n",
    "                self.local_cache[query] = wiki_chain.invoke(inputs)\n",
    "                return self.local_cache[query]\n",
    "        else:\n",
    "            last_query = list(self.local_cache.keys())[-1]\n",
    "            last_response = list(self.local_cache.values())[-1]\n",
    "\n",
    "            template2 = \"\"\"You are given a question and its context. If the context and question do not match in terms of the topic, then ignore the context and answer to the best of your knowledge. Otherwise, answer the question using the context provided.\n",
    "            You are also given the last query the user asked and the response given to that query given by you. You are expected to have context of this query when responding to the question. However, use the context only if it matches the question.\n",
    "            Previous Query: {prev_prompt}\n",
    "            Previous Response: {prev_response}\n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Answer:\"\"\"\n",
    "            prompt2 = PromptTemplate(\n",
    "                template=template2,\n",
    "                input_variables=[\"context\", \"question\", \"prev_prompt\", \"prev_response\"]\n",
    "            )\n",
    "            gen_chain = prompt2 | self.llm | StrOutputParser()\n",
    "\n",
    "            template3 = \"\"\"Using the following context answer the question. Do not make stuff from your on.\n",
    "            You are also given the last query the user asked and the response given to that query. You are expected to have context of this query and response.\n",
    "            Previous Query: {prev_prompt}\n",
    "            Previous Response: {prev_response}\n",
    "            Question: {question}\n",
    "            Context: {wiki_context}\n",
    "            Answer:\"\"\"\n",
    "            prompt3 = PromptTemplate(\n",
    "                template=template3,\n",
    "                input_variables=[\"wiki_context\", \"question\", \"prev_prompt\", \"prev_response\"]\n",
    "            )\n",
    "            wiki_chain = prompt3 | self.llm | StrOutputParser()\n",
    "            \n",
    "            condition = check_string(query)\n",
    "            if condition == \"No\":\n",
    "                # print(\"General/Context - History\")\n",
    "                # print(\"Last Response: \", last_response)\n",
    "                article = self.retriever.invoke(query)\n",
    "                inputs = {\n",
    "                    \"context\": article,\n",
    "                    \"question\": query,\n",
    "                    \"prev_prompt\": last_query,\n",
    "                    \"prev_response\": last_response\n",
    "                }\n",
    "                self.local_cache[query] = gen_chain.invoke(inputs)\n",
    "                return gen_chain.invoke(inputs)\n",
    "            else:\n",
    "                # print(\"Wikipedia - History\")\n",
    "                article = self.wiki_retriever_instance.invoke(condition)\n",
    "                inputs = {\n",
    "                    \"wiki_context\": article,\n",
    "                    \"question\": query,\n",
    "                    \"prev_prompt\": last_query,\n",
    "                    \"prev_response\": last_response\n",
    "                }\n",
    "                self.local_cache[query] = wiki_chain.invoke(inputs)\n",
    "                return self.local_cache[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing your ResearchChatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Dell\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bot = ResearchChatbot()\n",
    "bot.load_papers(\"./papers/\")\n",
    "bot.store_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Priory of Sion is a fraternal organization founded by Pierre Plantard in 1956 in Annemasse, Haute-Savoie, France. Plantard claimed that the Priory of Sion was the latest front for a secret society founded by crusading knight Godfrey of Bouillon in 1099, with the goal of installing a secret bloodline of the Merovingian dynasty on the thrones of France and the rest of Europe. These claims were later popularized by the authors of the 1982 book The Holy Blood and the Holy Grail and borrowed by Dan Brown for his 2003 mystery thriller novel The Da Vinci Code. However, it was later discovered that the historical existence and activities of the Priory of Sion before 1956 were a hoax created by Plantard as part of his unsuccessful attempt to become a respected, influential and wealthy player in French esotericist and monarchist circles. Despite being debunked as France's greatest 20th-century literary hoax by journalists and scholars, many conspiracy theorists still believe that the Priory of Sion was a millennium-old cabal concealing a religiously subversive secret. Some skeptics express concern that the proliferation and popularity of pseudohistorical books, websites and films inspired by the Priory of Sion hoax contribute to the problem of unfounded conspiracy theories becoming mainstream; while others are troubled by how these works romanticize the reactionary ideologies of the far right.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the `Priory of Sion`. Get information from Wikipedia.\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mixed-Domain Pretraining is a method of language model pretraining where out-domain text is assumed to be helpful and domain-specific pretraining is typically initialized with a general-domain language model and inherits its vocabulary. It is different from Domain-Specific Pretraining from Scratch which derives the vocabulary and conducts pretraining using solely in-domain text. According to the study mentioned in the context, for domains with abundant text such as biomedicine, Domain-Specific Pretraining from Scratch can substantially outperform the conventional mixed-domain approach. The authors of the study also question the prevailing assumption that out-domain text is still helpful and suggest that for domains with abundant unlabeled text such as biomedicine, it is unclear that domain-specific pretraining can benefit from transfer from general domains. They also express concern about negative transfer that actually hinders the target performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you explain to me what is Mixed-Domain Pretraining?\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the previous answer, I explained that Mixed-Domain Pretraining involves using out-domain text in addition to domain-specific text for language model pretraining, while Domain-Specific Pretraining from Scratch uses only in-domain text. I also mentioned a study that found that for domains with abundant text such as biomedicine, Domain-Specific Pretraining from Scratch can outperform the conventional mixed-domain approach, and raised concerns about negative transfer from general domains to specific domains.\n"
     ]
    }
   ],
   "source": [
    "query = \"Could you please summarize your previous answer in one line?\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Priory of Sion is a fraternal organization founded by Pierre Plantard in 1956 in Annemasse, Haute-Savoie, France. Plantard claimed that the Priory of Sion was the latest front for a secret society founded by crusading knight Godfrey of Bouillon in 1099, with the goal of installing a secret bloodline of the Merovingian dynasty on the thrones of France and the rest of Europe. These claims were later popularized by the authors of the 1982 book The Holy Blood and the Holy Grail and borrowed by Dan Brown for his 2003 mystery thriller novel The Da Vinci Code. However, it was later discovered that the historical existence and activities of the Priory of Sion before 1956 were a hoax created by Plantard as part of his unsuccessful attempt to become a respected, influential and wealthy player in French esotericist and monarchist circles. Despite being debunked as France's greatest 20th-century literary hoax by journalists and scholars, many conspiracy theorists still believe that the Priory of Sion was a millennium-old cabal concealing a religiously subversive secret. Some skeptics express concern that the proliferation and popularity of pseudohistorical books, websites and films inspired by the Priory of Sion hoax contribute to the problem of unfounded conspiracy theories becoming mainstream; while others are troubled by how these works romanticize the reactionary ideologies of the far right.\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"What is the `Priory of Sion`. Get information from Wikipedia.\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The Priory of Sion is a fraternal organization founded in France, which was claimed to be a centuries-old cabal concealing a religiously subversive secret, but was later exposed as a hoax.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain previous answer in one line.\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The biggest mammal is the blue whale.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the biggest mammal?\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Blue whales are typically found in the ocean. They are known to inhabit a wide range of areas including the Arctic, Antarctic, North Atlantic, and North Pacific oceans. They prefer deep, offshore waters and are known to migrate long distances for feeding and breeding. However, specific locations can vary depending on the time of year and other environmental factors.\n"
     ]
    }
   ],
   "source": [
    "query = \"Where is it found?\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
